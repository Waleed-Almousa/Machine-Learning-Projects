{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8163daf4",
   "metadata": {},
   "source": [
    "### HW04: Practice with feature engineering, splitting data, and fitting and regularizing linear models\n",
    "\n",
    "Waleed Almousa\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdedd9b",
   "metadata": {},
   "source": [
    "### Hello Students:\n",
    "\n",
    "- Start by downloading HW04.ipynb from this folder. Then develop it into your solution.\n",
    "- Write code where you see \"... your code here ...\" below.\n",
    "  (You are welcome to use more than one cell.)\n",
    "- If you have questions, please ask them in class or office hours. Our TA\n",
    "  and I are very happy to help with the programming (provided you start early\n",
    "  enough, and provided we are not helping so much that we undermine your learning).\n",
    "- When you are done, run these Notebook commands:\n",
    "  - Shift-L (once, so that line numbers are visible)\n",
    "  - Kernel > Restart and Run All (run all cells from scratch)\n",
    "  - Esc S (save)\n",
    "  - File > Download as > HTML\n",
    "- Turn in:\n",
    "  - HW04.ipynb to Canvas's HW04.ipynb assignment\n",
    "  - HW04.html to Canvas's HW04.html assignment\n",
    "  - As a check, download your files from Canvas to a new 'junk' folder. Try 'Kernel > Restart\n",
    "  and Run All' on the '.ipynb' file to make sure it works. Glance through the '.html' file.\n",
    "- Turn in partial solutions to Canvas before the deadline. e.g. Turn in part 1,\n",
    "  then parts 1 and 2, then your whole solution. That way we can award partial credit\n",
    "  even if you miss the deadline. We will grade your last submission before the deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6a87448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90946b9",
   "metadata": {},
   "source": [
    "## 1. Feature engineering (one-hot encoding and data imputation)\n",
    "\n",
    "(Note: This paragraph is not instructions but rather is to communicate context for this exercise. We use the same Titanic data we used in HW02:\n",
    "- There we used `df.dropna()` to drop any observations with missing values; here we use data imputation instead.\n",
    "- There we manually did one-hot encoding of the categorical `Sex` column by making a `Female` column; here we do the same one-hot encoding with the help of pandas's `df.join(pd.get_dummies())`.\n",
    "- There we used a decision tree; here we use $k$-NN.\n",
    "\n",
    "We evaluate how these strategies can improve model performance by allowing us to use columns with categorical or missing data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa128a8c",
   "metadata": {},
   "source": [
    "### 1a. Read the data from [http://www.stat.wisc.edu/~jgillett/451/data/kaggle_titanic_train.csv](http://www.stat.wisc.edu/~jgillett/451/data/kaggle_titanic_train.csv).\n",
    "- Retain only these columns: Survived, Pclass, Sex, Age, SibSp, Parch.\n",
    "- Display the first 7 rows.\n",
    "\n",
    "These data are described at [https://www.kaggle.com/competitions/titanic/data](https://www.kaggle.com/competitions/titanic/data) (click on the small down-arrow to see the \"Data Dictionary\"), which is where they are from.\n",
    "- Read that \"Data Dictionary\" paragraph (with your eyes, not python) so you understand what each column represents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "700852b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=pd.read_csv(\"http://www.stat.wisc.edu/~jgillett/451/data/kaggle_titanic_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "308788f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age  SibSp  Parch\n",
       "0         0       3    male  22.0      1      0\n",
       "1         1       1  female  38.0      1      0\n",
       "2         1       3  female  26.0      0      0\n",
       "3         1       1  female  35.0      1      0\n",
       "4         0       3    male  35.0      0      0\n",
       "5         0       3    male   NaN      0      0\n",
       "6         0       1    male  54.0      0      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=raw_data[[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\"]]\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fda2ee",
   "metadata": {},
   "source": [
    "### 1b. Try to train a $k$NN model to predict $y=$ 'Survived' from $X=$ these features: 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch'.\n",
    "- Use $k = 3$ and the (default) euclidean metric.\n",
    "- Notice at the bottom of the error message that it fails with the error \"ValueError: could not convert string to float: 'male'\".\n",
    "- Comment out your .fit() line so the cell can run without error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf87ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf=KNeighborsClassifier(n_neighbors=3)\n",
    "X=df[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\"]]\n",
    "y=df.Survived\n",
    "\n",
    "# clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eea90e",
   "metadata": {},
   "source": [
    "### 1c. Try to train again, this time without the 'Sex' feature.\n",
    "- Notice that it fails because \"Input contains NaN\".\n",
    "- Comment out your .fit() line so the cell can run without error.\n",
    "- Run `X.isna().any()` (where X is the name of your DataFrame of features) to see that\n",
    "  the 'Age' feature has missing values. (You can see the first missing value in\n",
    "  the sixth row that you displayed above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4032cc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass    False\n",
       "Age        True\n",
       "SibSp     False\n",
       "Parch     False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "clf=KNeighborsClassifier(n_neighbors=3)\n",
    "X=df[[\"Pclass\", \"Age\", \"SibSp\", \"Parch\"]]\n",
    "y=df.Survived\n",
    "\n",
    "X.isna().any()\n",
    "# clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b103b9",
   "metadata": {},
   "source": [
    "### 1d. Train without the 'Sex' and 'Age' features.\n",
    "- Report accuracy on the training data with a line of the form\n",
    "  `Accuracy on training data is  0.500` (0.500 may not be correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e626bdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data is 0.664\n"
     ]
    }
   ],
   "source": [
    "clf=KNeighborsClassifier(n_neighbors=3)\n",
    "X=df[[\"Pclass\", \"SibSp\", \"Parch\"]]\n",
    "y=df.Survived\n",
    "clf.fit(X, y)\n",
    "training_acc=clf.score(X, y)\n",
    "print(f\"Accuracy on training data is {training_acc:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b617602",
   "metadata": {},
   "source": [
    "### 1e.  Use one-hot encoding\n",
    "to include a binary 'male'  feature made from the 'Sex' feature. (Or include a binary 'female'\n",
    "feature, according to your preference. Using both is unnecessary since either is the logical\n",
    "negation of the other.) That is, train on these features: 'Pclass', 'SibSp', 'Parch', 'male'.\n",
    "- Use pandas's df.join(pd.get_dummies())`.\n",
    "- Report training accuracy as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71d04e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data is 0.744\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df=df.join(pd.get_dummies(df.Sex, prefix=\"male\"))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "clf=KNeighborsClassifier(n_neighbors=3)\n",
    "X=df[[\"Pclass\", \"SibSp\", \"Parch\", \"male_male\"]]\n",
    "y=df.Survived\n",
    "clf.fit(X, y)\n",
    "\n",
    "training_acc=clf.score(X, y)\n",
    "print(f\"Accuracy on training data is {training_acc:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210618af",
   "metadata": {},
   "source": [
    "### 1f. Use data imputation\n",
    "to include an 'age' feature made from 'Age' but replacing each missing value with the median\n",
    "of the non-missing ages. That is, train on these features: 'Pclass', 'SibSp', 'Parch', 'male',\n",
    "'age'.\n",
    "\n",
    "- Report training accuracy as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0753fb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data is 0.863\n"
     ]
    }
   ],
   "source": [
    "imp = SimpleImputer(missing_values=np.nan, strategy='median', fill_value=None)\n",
    "df[\"age\"]=imp.fit_transform(df[['Age']])\n",
    "clf=KNeighborsClassifier(n_neighbors=3)\n",
    "X=df[[\"Pclass\", \"SibSp\", \"Parch\", \"male_male\", \"age\"]]\n",
    "y=df.Survived\n",
    "clf.fit(X, y)\n",
    "\n",
    "training_acc=clf.score(X, y)\n",
    "print(f\"Accuracy on training data is {training_acc:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9050084a",
   "metadata": {},
   "source": [
    "## 2. Explore model fit, overfit, and regularization in the context of multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715fc1b2",
   "metadata": {},
   "source": [
    "### 2a. Prepare the data:\n",
    "- Read [http://www.stat.wisc.edu/~jgillett/451/data/mtcars.csv](http://www.stat.wisc.edu/~jgillett/451/data/mtcars.csv) into a DataFrame.\n",
    "- Set a variable `X` to the subset consisting of all columns except `mpg`.\n",
    "- Set a variable `y` to the `mpg` column.\n",
    "- Use `train_test_split()` to split `X` and `y` into `X_train`, `X_test`, `y_train`, and `y_test`.\n",
    "  - Reserve half the data for training and half for testing.\n",
    "  - Use `random_state=0` to get reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbf49fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=pd.read_csv(\"http://www.stat.wisc.edu/~jgillett/451/data/mtcars.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30732e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=raw_data.drop(columns=[\"mpg\", \"Unnamed: 0\"])\n",
    "y=raw_data.mpg\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1b3e11",
   "metadata": {},
   "source": [
    "### 2b. Train three models on the training data and evaluate each on the test data:\n",
    "- `LinearRegression()`\n",
    "- `Lasso()`\n",
    "- `Ridge()`\n",
    "\n",
    "The evaluation consists in displaying MSE$_\\text{train}, $ MSE$_\\text{test}$, and the coefficients $\\mathbf{w}$ for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb40699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINEAR REGRESSION MSE_train: 0.386\n",
      "LINEAR REGRESSION MSE_test: 30.2\n",
      "LINEAR REGRESSION coef_: [ 1.08241627  0.02500827  0.02593759  3.12997296 -7.34326296  3.93233873\n",
      " -4.08551838 -1.2164054   5.98718293 -0.77097967]\n",
      "\n",
      "LASSO MSE_train: 5.69\n",
      "LASSO MSE_test: 13.0\n",
      "LASSO coef_: [-0.         -0.03718773 -0.01681757  0.         -0.          0.\n",
      "  0.          0.          0.         -0.84712891]\n",
      "\n",
      "RIDGE MSE_train: 1.99\n",
      "RIDGE MSE_test: 11.2\n",
      "RIDGE coef_: [-0.01392446 -0.00744628  0.00400324  0.79271685 -3.21854592  1.09825372\n",
      " -0.48236992  0.47079795  1.49546846 -1.04547895]\n"
     ]
    }
   ],
   "source": [
    "LR= linear_model.LinearRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "LR_MSE_train = (1/y_train.size) * np.sum((y_train - LR.predict(X_train))**2)\n",
    "LR_MSE_test = (1/y_test.size)  * np.sum((y_test - LR.predict(X_test))**2)\n",
    "LR_w=LR.coef_\n",
    "\n",
    "print(f\"LINEAR REGRESSION MSE_train: {LR_MSE_train:.3}\")\n",
    "print(f\"LINEAR REGRESSION MSE_test: {LR_MSE_test:.3}\")\n",
    "print(f\"LINEAR REGRESSION coef_: {LR_w}\")\n",
    "\n",
    "lasso= linear_model.Lasso()\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso_MSE_train = (1/y_train.size) * np.sum((y_train - lasso.predict(X_train))**2)\n",
    "lasso_MSE_test = (1/y_test.size)  * np.sum((y_test - lasso.predict(X_test))**2)\n",
    "lasso_w=lasso.coef_\n",
    "\n",
    "print(f\"\\nLASSO MSE_train: {lasso_MSE_train:.3}\")\n",
    "print(f\"LASSO MSE_test: {lasso_MSE_test:.3}\")\n",
    "print(f\"LASSO coef_: {lasso_w}\")\n",
    "\n",
    "ridge= linear_model.Ridge()\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge_MSE_train = (1/y_train.size) * np.sum((y_train - ridge.predict(X_train))**2)\n",
    "ridge_MSE_test = (1/y_test.size)  * np.sum((y_test - ridge.predict(X_test))**2)\n",
    "ridge_w=ridge.coef_\n",
    "\n",
    "print(f\"\\nRIDGE MSE_train: {ridge_MSE_train:.3}\")\n",
    "print(f\"RIDGE MSE_test: {ridge_MSE_test:.3}\")\n",
    "print(f\"RIDGE coef_: {ridge_w:}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523ff587",
   "metadata": {},
   "source": [
    "### 2c. Answer a few questions about the models:\n",
    "- Which one best fits the training data?\n",
    "- Which one best fits the test data?\n",
    "- Which one does feature selection by setting most coefficients to zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c91d00",
   "metadata": {},
   "source": [
    "\n",
    "Linear regression best fits the training data\n",
    "\n",
    "Ridge best fits the test data\n",
    "\n",
    "Lasso does feature selection by setting most coefficients to zero\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
